{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:22.986447554Z",
     "start_time": "2023-08-24T10:07:22.010698523Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from lllm import Suspect\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:23.009831170Z",
     "start_time": "2023-08-24T10:07:22.660881009Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:26.725538021Z",
     "start_time": "2023-08-24T10:07:22.667386783Z"
    }
   },
   "outputs": [],
   "source": [
    "from lllm.questions_loaders import SyntheticFacts, Questions1000, WikiData, Commonsense2, TatoebaEngToFre, \\\n",
    "    TatoebaFreToEng, Sciq, MathematicalProblems, AnthropicAwarenessAI, AnthropicAwarenessArchitecture, \\\n",
    "    AnthropicAwarenessNNArchitecture\n",
    "\n",
    "# all datasets\n",
    "datasets = [SyntheticFacts(), Questions1000(), WikiData(), Commonsense2(), TatoebaFreToEng(), TatoebaEngToFre(),\n",
    "            Sciq(), MathematicalProblems(), AnthropicAwarenessAI(), AnthropicAwarenessArchitecture(),\n",
    "            AnthropicAwarenessNNArchitecture()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.396888672Z",
     "start_time": "2023-08-24T10:07:26.726910171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1254, 1328)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lie_transcripts = []\n",
    "truth_transcripts = []\n",
    "for dataset in datasets:\n",
    "    lie_transcripts.extend(dataset.get_lying_transcripts())\n",
    "    truth_transcripts.extend(dataset.get_truthful_transcripts())\n",
    "\n",
    "rng.shuffle(lie_transcripts)\n",
    "rng.shuffle(truth_transcripts)\n",
    "\n",
    "len(lie_transcripts), len(truth_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.397469736Z",
     "start_time": "2023-08-24T10:07:26.850713484Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_prompt(transcript):\n",
    "    \n",
    "    return f'Question: {transcript[0]}\\nAnswer: {transcript[1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.535799329Z",
     "start_time": "2023-08-24T10:07:26.869131215Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "YES_TOKENS = set(['yes', 'absolutely', 'certainly', 'undoubtedly'])\n",
    "NO_TOKENS = set(['no', 'never'])\n",
    "def process_logprobs(logprobs):\n",
    "    # first, remove all before the *out loud* tokens (if present), for each element in the logprobs\n",
    "    logprob_differences_list = []\n",
    "    prob_differences_list = []\n",
    "    for logprob in logprobs:\n",
    "        produced_tokens = logprob[\"tokens\"]\n",
    "        # now loop over the tokens and see if there is a \"loud\" followed by a \"*\" token\n",
    "        found = 0\n",
    "        for i in range(len(produced_tokens) - 1):\n",
    "            if \"loud\" in produced_tokens[i] and \"*\" in produced_tokens[i + 1]:\n",
    "                found = i + 2\n",
    "                break\n",
    "\n",
    "        # part of the following adapted from Ilan's code\n",
    "        actual_logprobs = list(logprob[\"logprobs\"][found].values())\n",
    "        logprob_tokens = list(logprob[\"logprobs\"][found].keys())\n",
    "        logprob_tokens = [t.lower().strip() for t in logprob_tokens]\n",
    "\n",
    "        yes_logsum = None\n",
    "        no_logsum = None\n",
    "        # now we consider the actual_logprobs at that position, sum those corresponding to \"yes\" and subtract those corresponding to \"no\"\n",
    "        for tok, prob in zip(logprob_tokens, actual_logprobs):\n",
    "            if tok in YES_TOKENS:\n",
    "                yes_logsum = np.logaddexp(yes_logsum, prob) if yes_logsum is not None else prob\n",
    "            elif tok in NO_TOKENS:\n",
    "                no_logsum = np.logaddexp(no_logsum, prob) if no_logsum is not None else prob\n",
    "\n",
    "        # if the probability of either yes or no is 0 bound it with the minimum of the probabilities in\n",
    "        # the list (as those are the top 5) or with 1 - sum of the probabilities in the list (as that bounds\n",
    "        # the overall prob assigned to the missing tokens).\n",
    "        if yes_logsum is None:\n",
    "            yes_logsum = min(min(actual_logprobs), np.log(1 - np.exp(logsumexp(actual_logprobs))))\n",
    "        if no_logsum is None:\n",
    "            no_logsum = min(min(actual_logprobs), np.log(1 - np.exp(logsumexp(actual_logprobs))))\n",
    "\n",
    "        # should I consider the difference between logprobs or between probs?\n",
    "        logprob_differences_list.append(yes_logsum - no_logsum)\n",
    "        prob_differences_list.append(np.exp(yes_logsum) - np.exp(no_logsum))\n",
    "\n",
    "    return logprob_differences_list, prob_differences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate logprobs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "probes = pd.read_csv(\"../../data/probes.csv\")[\"probe\"].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.671814768Z",
     "start_time": "2023-08-24T10:07:26.897195378Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model_kwargs = {\"temperature\": 0.7, \"presence_penalty\": 0, \"frequency_penalty\": 0,\n",
    "                \"max_tokens\": 128, \"top_p\": 1, \"stop\": \"\\n\", \"logprobs\": 5}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.788225516Z",
     "start_time": "2023-08-24T10:07:26.934712927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "RUN = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.790358463Z",
     "start_time": "2023-08-24T10:07:26.972109358Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.854485904Z",
     "start_time": "2023-08-24T10:07:27.013756689Z"
    }
   },
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    logprob_differences_lie = []\n",
    "    for transcript in tqdm(lie_transcripts[:20]):\n",
    "        prompt = make_prompt(transcript)\n",
    "        suspect = Suspect(prompt, n_parallel=len(probes), model='text-davinci-003', model_kwargs=model_kwargs)\n",
    "        answer,logprobs = suspect.get_response(probes, return_logprobs=True)\n",
    "        logprob_differences_list, prob_differences_list = process_logprobs(logprobs)\n",
    "        logprob_differences_lie.append(logprob_differences_list)\n",
    "\n",
    "    logprob_differences_lie = np.array(logprob_differences_lie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.854889567Z",
     "start_time": "2023-08-24T10:07:27.058990448Z"
    }
   },
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    logprob_differences_true = []\n",
    "    for transcript in tqdm(truth_transcripts[:20]):\n",
    "        prompt = make_prompt(transcript)\n",
    "        suspect = Suspect(prompt, n_parallel=len(probes), model='text-davinci-003', model_kwargs=model_kwargs)\n",
    "        answer,logprobs = suspect.get_response(probes,return_logprobs=True)\n",
    "        logprob_differences_list, prob_differences_list = process_logprobs(logprobs)\n",
    "        logprob_differences_true.append(logprob_differences_list)\n",
    "\n",
    "    logprob_differences_true = np.array(logprob_differences_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    # save to disk\n",
    "    np.save(\"../../results/logprob_differences_lie_conversation_only.npy\",logprob_differences_lie)\n",
    "    np.save(\"../../results/logprob_differences_true_conversation_only.npy\",logprob_differences_true)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.855124719Z",
     "start_time": "2023-08-24T10:07:27.102970805Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# load from disk\n",
    "logprob_differences_lie = np.load(\"../../results/logprob_differences_lie_conversation_only.npy\")\n",
    "logprob_differences_true = np.load(\"../../results/logprob_differences_true_conversation_only.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:27.856098740Z",
     "start_time": "2023-08-24T10:07:27.103348656Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "with open(\"../../results/trained_classifiers/logistic_logprobs_classifier_all_probes.pkl\", \"rb\") as f:\n",
    "    classifier_all = pickle.load(f)\n",
    "with open(\"../../results/trained_classifiers/logistic_logprobs_classifier_no_lie.pkl\", \"rb\") as f:\n",
    "    classifier_no_lie = pickle.load(f)\n",
    "with open(\"../../results/trained_classifiers/logistic_logprobs_classifier_lie.pkl\", \"rb\") as f:\n",
    "    classifier_lie = pickle.load(f)\n",
    "with open(\"../../results/trained_classifiers/logistic_logprobs_classifier_knowable.pkl\", \"rb\") as f:\n",
    "    classifier_knowable = pickle.load(f)\n",
    "\n",
    "classifiers_logprobs_list = [classifier_all, classifier_no_lie,\n",
    "                             classifier_lie, classifier_knowable]\n",
    "\n",
    "# same for binary instead of logprobs:\n",
    "with open(\"../../results/trained_classifiers/logistic_binary_classifier_all_probes.pkl\", \"rb\") as f:\n",
    "    classifier_all_binary = pickle.load(f)\n",
    "with open(\"../../results/trained_classifiers/logistic_binary_classifier_no_lie.pkl\", \"rb\") as f:\n",
    "    classifier_no_lie_binary = pickle.load(f)\n",
    "with open(\"../../results/trained_classifiers/logistic_binary_classifier_lie.pkl\", \"rb\") as f:\n",
    "    classifier_lie_binary = pickle.load(f)\n",
    "with open(\"../../results/trained_classifiers/logistic_binary_classifier_knowable.pkl\", \"rb\") as f:\n",
    "    classifier_knowable_binary = pickle.load(f)\n",
    "\n",
    "classifiers_binary_list = [classifier_all_binary, classifier_no_lie_binary,\n",
    "                          classifier_lie_binary, classifier_knowable_binary]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:30.113713652Z",
     "start_time": "2023-08-24T10:07:27.146928938Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(probes))\n",
    "no_lie_indices = np.load(\"../../results/probes_groups/no_lie_indices.npy\")\n",
    "lie_indices = np.load(\"../../results/probes_groups/lie_indices.npy\")\n",
    "knowable_indices = np.load(\"../../results/probes_groups/knowable_indices.npy\")\n",
    "\n",
    "indeces_list = [all_indices, no_lie_indices, lie_indices, knowable_indices]\n",
    "\n",
    "all_indices = np.arange(len(probes))\n",
    "no_lie_indices_binary = np.load(\"../../results/probes_groups/no_lie_indices.npy\")\n",
    "lie_indices_binary = np.load(\"../../results/probes_groups/lie_indices.npy\")\n",
    "knowable_indices_binary = np.load(\"../../results/probes_groups/knowable_indices.npy\")\n",
    "\n",
    "indeces_list_binary = [all_indices, no_lie_indices_binary,\n",
    "                       lie_indices_binary, knowable_indices_binary]\n",
    "\n",
    "name_list = [\"all_probes\", \"no_lie\", \"lie\", \"knowable\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:30.141456810Z",
     "start_time": "2023-08-24T10:07:29.071536924Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "logprobs = np.concatenate([logprob_differences_true, logprob_differences_lie])\n",
    "labels = np.concatenate([np.ones(len(logprob_differences_true)), np.zeros(len(logprob_differences_lie))])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:30.141871438Z",
     "start_time": "2023-08-24T10:07:29.089170181Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logprobs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  probe_subset  accuracy     auc\n0   all_probes      0.50  0.7100\n0       no_lie      0.55  0.6475\n0          lie      0.50  0.6200\n0     knowable      0.50  0.6025",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>probe_subset</th>\n      <th>accuracy</th>\n      <th>auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>all_probes</td>\n      <td>0.50</td>\n      <td>0.7100</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>no_lie</td>\n      <td>0.55</td>\n      <td>0.6475</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>lie</td>\n      <td>0.50</td>\n      <td>0.6200</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>knowable</td>\n      <td>0.50</td>\n      <td>0.6025</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=[\"probe_subset\", \"accuracy\", \"auc\", \"y_pred\", \"y_pred_proba\"])\n",
    "\n",
    "for classifier, indeces, name in zip(classifiers_logprobs_list, indeces_list, name_list):\n",
    "    accuracy, auc, _, y_pred, y_pred_proba = classifier.evaluate(logprobs[:,indeces], labels, return_ys=True)\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame(\n",
    "        {\"probe_subset\": [name], \"accuracy\": [accuracy], \"auc\": [auc], \"y_pred\": [y_pred],\n",
    "         \"y_pred_proba\": [y_pred_proba]}) ])\n",
    "results_df[[\"probe_subset\", \"accuracy\", \"auc\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:30.142968225Z",
     "start_time": "2023-08-24T10:07:29.119314240Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Binary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  probe_subset  accuracy      auc\n0   all_probes       0.5  0.51500\n0       no_lie       0.5  0.46125\n0          lie       0.5  0.50250\n0     knowable       0.5  0.55000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>probe_subset</th>\n      <th>accuracy</th>\n      <th>auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>all_probes</td>\n      <td>0.5</td>\n      <td>0.51500</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>no_lie</td>\n      <td>0.5</td>\n      <td>0.46125</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>lie</td>\n      <td>0.5</td>\n      <td>0.50250</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>knowable</td>\n      <td>0.5</td>\n      <td>0.55000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_binary = pd.DataFrame(columns=[\"probe_subset\", \"accuracy\", \"auc\", \"y_pred\", \"y_pred_proba\"])\n",
    "\n",
    "for classifier, indeces, name in zip(classifiers_binary_list, indeces_list_binary, name_list):\n",
    "    accuracy, auc, _, y_pred, y_pred_proba = classifier.evaluate(logprobs[:,indeces]>0, labels, return_ys=True)\n",
    "\n",
    "    results_df_binary = pd.concat([results_df_binary, pd.DataFrame(\n",
    "        {\"probe_subset\": [name], \"accuracy\": [accuracy], \"auc\": [auc], \"y_pred\": [y_pred],\n",
    "         \"y_pred_proba\": [y_pred_proba]}) ])\n",
    "results_df_binary[[\"probe_subset\", \"accuracy\", \"auc\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:30.144620885Z",
     "start_time": "2023-08-24T10:07:29.206986374Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
